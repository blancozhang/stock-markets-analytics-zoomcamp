{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725436bd-7e7d-4759-b4dc-a07c55ee1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e7b68-4211-4cb6-9ffa-a6b57bb6b542",
   "metadata": {},
   "source": [
    "Question 1: [Index] S&P 500 Stocks Added to the Index\n",
    "Which year had the highest number of additions?\n",
    "\n",
    "Using the list of S&P 500 companies from Wikipedia's S&P 500 companies page, download the data including the year each company was added to the index.\n",
    "\n",
    "Hint: you can use pandas.read_html to scrape the data into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Create a DataFrame with company tickers, names, and the year they were added.\n",
    "Extract the year from the addition date and calculate the number of stocks added each year.\n",
    "Which year had the highest number of additions (1957 doesn't count, as it was the year when the S&P 500 index was founded)? Write down this year as your answer (the most recent one, if you have several records).\n",
    "Context:\n",
    "\n",
    "\"Following the announcement, all four new entrants saw their stock prices rise in extended trading on Friday\" - recent examples of S&P 500 additions include DASH, WSM, EXE, TKO in 2025 (Nasdaq article).\n",
    "\n",
    "Additional: How many current S&P 500 stocks have been in the index for more than 20 years? When stocks are added to the S&P 500, they usually experience a price bump as investors and index funds buy shares following the announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f27717-8120-4c5b-888b-bef38c54791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b862dc99-9d41-4f61-8ce6-cf669571849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url, match='Date added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d1c3e3-4f46-4fbb-b5e3-0a206e7a96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The desired table is usually the first one that matches\n",
    "df = tables[0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b233eb-8894-4878-9736-5456c135f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>XYL</td>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Machinery &amp; Supplies &amp; Components</td>\n",
       "      <td>White Plains, New York</td>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>1524472</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>1997-10-06</td>\n",
       "      <td>1041061</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Electronic Equipment &amp; Instruments</td>\n",
       "      <td>Lincolnshire, Illinois</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>877212</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Warsaw, Indiana</td>\n",
       "      <td>2001-08-07</td>\n",
       "      <td>1136869</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Parsippany, New Jersey</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>1555280</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol             Security             GICS Sector  \\\n",
       "0      MMM                   3M             Industrials   \n",
       "1      AOS          A. O. Smith             Industrials   \n",
       "2      ABT  Abbott Laboratories             Health Care   \n",
       "3     ABBV               AbbVie             Health Care   \n",
       "4      ACN            Accenture  Information Technology   \n",
       "..     ...                  ...                     ...   \n",
       "498    XYL           Xylem Inc.             Industrials   \n",
       "499    YUM          Yum! Brands  Consumer Discretionary   \n",
       "500   ZBRA   Zebra Technologies  Information Technology   \n",
       "501    ZBH        Zimmer Biomet             Health Care   \n",
       "502    ZTS               Zoetis             Health Care   \n",
       "\n",
       "                                GICS Sub-Industry    Headquarters Location  \\\n",
       "0                        Industrial Conglomerates    Saint Paul, Minnesota   \n",
       "1                               Building Products     Milwaukee, Wisconsin   \n",
       "2                           Health Care Equipment  North Chicago, Illinois   \n",
       "3                                   Biotechnology  North Chicago, Illinois   \n",
       "4                  IT Consulting & Other Services          Dublin, Ireland   \n",
       "..                                            ...                      ...   \n",
       "498  Industrial Machinery & Supplies & Components   White Plains, New York   \n",
       "499                                   Restaurants     Louisville, Kentucky   \n",
       "500            Electronic Equipment & Instruments   Lincolnshire, Illinois   \n",
       "501                         Health Care Equipment          Warsaw, Indiana   \n",
       "502                               Pharmaceuticals   Parsippany, New Jersey   \n",
       "\n",
       "     Date added      CIK      Founded  \n",
       "0    1957-03-04    66740         1902  \n",
       "1    2017-07-26    91142         1916  \n",
       "2    1957-03-04     1800         1888  \n",
       "3    2012-12-31  1551152  2013 (1888)  \n",
       "4    2011-07-06  1467373         1989  \n",
       "..          ...      ...          ...  \n",
       "498  2011-11-01  1524472         2011  \n",
       "499  1997-10-06  1041061         1997  \n",
       "500  2019-12-23   877212         1969  \n",
       "501  2001-08-07  1136869         1927  \n",
       "502  2013-06-21  1555280         1952  \n",
       "\n",
       "[503 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9878436e-395b-4770-801b-aa1a678ffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easier access\n",
    "df.rename(columns={'Date added': 'Date_added'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becef25e-ad8d-4d24-b7fa-718c2526dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date_added' to datetime objects and extract the year\n",
    "# use errors='coerce' to turn unparseable dates into NaT (Not a Time)\n",
    "df['Year_added'] = pd.to_datetime(df['Date_added'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618a2511-c18d-4e14-9a9a-cd5ded7c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Year_added' is NaN (dates that couldn't be parsed)\n",
    "df.dropna(subset=['Year_added'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb543a8-2868-438b-8a2a-bdc24821750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Year_added' to integer type\n",
    "df['Year_added'] = df['Year_added'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2ebbec-a502-4e30-a662-2f69240f2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of stocks added each year\n",
    "additions_per_year = df['Year_added'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d84e39-ad30-427b-a601-229dfdb19045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 1957\n",
    "additions_per_year = additions_per_year[additions_per_year.index != 1957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c05e2b-7dec-4b14-abab-5837fd9ef778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year with the highest number of additions (excluding 1957) is: 2017\n"
     ]
    }
   ],
   "source": [
    "# Find the year with the highest number of additions\n",
    "\n",
    "highest_additions_year = additions_per_year.loc[additions_per_year == additions_per_year.max()].index.max()\n",
    "\n",
    "\n",
    "print(f\"The year with the highest number of additions (excluding 1957) is: {highest_additions_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d088a4ce-3e95-47f5-bc6e-5398e3195dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548d6a65-7760-44e8-9e4b-5dbad83ea974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of current S&P 500 stocks that have been in the index for more than 20 years: 219\n"
     ]
    }
   ],
   "source": [
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Calculate the tenure in years\n",
    "df['Tenure_Years'] = current_year - df['Year_added']\n",
    "\n",
    "# Count how many stocks have been in the index for more than 20 years\n",
    "stocks_more_than_20_years = df[df['Tenure_Years'] > 20].shape[0]\n",
    "\n",
    "print(f\"Number of current S&P 500 stocks that have been in the index for more than 20 years: {stocks_more_than_20_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73e0f1-9c8d-44b4-8940-ccb67630e57e",
   "metadata": {},
   "source": [
    "Question 2. [Macro] Indexes YTD (as of 1 May 2025)\n",
    "How many indexes (out of 10) have better year-to-date returns than the US (S&P 500) as of May 1, 2025?\n",
    "\n",
    "Using Yahoo Finance World Indices data, compare the year-to-date (YTD) performance (1 January-1 May 2025) of major stock market indexes for the following countries:\n",
    "\n",
    "United States - S&P 500 (^GSPC)\n",
    "China - Shanghai Composite (000001.SS)\n",
    "Hong Kong - HANG SENG INDEX (^HSI)\n",
    "Australia - S&P/ASX 200 (^AXJO)\n",
    "India - Nifty 50 (^NSEI)\n",
    "Canada - S&P/TSX Composite (^GSPTSE)\n",
    "Germany - DAX (^GDAXI)\n",
    "United Kingdom - FTSE 100 (^FTSE)\n",
    "Japan - Nikkei 225 (^N225)\n",
    "Mexico - IPC Mexico (^MXX)\n",
    "Brazil - Ibovespa (^BVSP)\n",
    "Hint: use start_date='2025-01-01' and end_date='2025-05-01' when downloading daily data in yfinance\n",
    "\n",
    "Context:\n",
    "\n",
    "Global Valuations: Who's Cheap, Who's Not? article suggests \"Other regions may be growing faster than the US and you need to diversify.\"\n",
    "\n",
    "Reference: Yahoo Finance World Indices - https://finance.yahoo.com/world-indices/\n",
    "\n",
    "Additional: How many of these indexes have better returns than the S&P 500 over 3, 5, and 10 year periods? Do you see the same trend? Note: For simplicity, ignore currency conversion effects.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a7e3a3-76aa-4781-b2b9-2668a99f0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active code page: 65001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script websockets.exe is installed in 'C:\\Users\\zclxs\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sample.exe is installed in 'C:\\Users\\zclxs\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.61-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\zclxs\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (4.2.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (2.4.2)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.1.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "     -------------------------------------- - 2.9/3.0 MB 27.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.0/3.0 MB 22.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.11.1-cp39-abi3-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yfinance) (4.25.3)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zclxs\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zclxs\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Downloading yfinance-0.2.61-py2.py3-none-any.whl (117 kB)\n",
      "Downloading curl_cffi-0.11.1-cp39-abi3-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 25.1 MB/s eta 0:00:00\n",
      "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.18.1-py3-none-any.whl size=139097 sha256=2e3970b30a434d2a7ab922c2c029992d687fc4ed28e19f64701530ef57e03778\n",
      "  Stored in directory: c:\\users\\zclxs\\appdata\\local\\pip\\cache\\wheels\\1a\\57\\6a\\bb71346381d0d911cd4ce3026f1fa720da76707e4f01cf27dd\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, websockets, curl_cffi, yfinance\n",
      "Successfully installed curl_cffi-0.11.1 multitasking-0.0.11 peewee-3.18.1 websockets-15.0.1 yfinance-0.2.61\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbdf4465-0f7b-427f-98e2-f057470f51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c537c2-11f3-4562-9b83-fb4444fd2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tickers for the world indices\n",
    "tickers = {\n",
    "    \"United States - S&P 500\": \"^GSPC\",\n",
    "    \"China - Shanghai Composite\": \"000001.SS\",\n",
    "    \"Hong Kong - HANG SENG INDEX\": \"^HSI\",\n",
    "    \"Australia - S&P/ASX 200\": \"^AXJO\",\n",
    "    \"India - Nifty 50\": \"^NSEI\",\n",
    "    \"Canada - S&P/TSX Composite\": \"^GSPTSE\",\n",
    "    \"Germany - DAX\": \"^GDAXI\",\n",
    "    \"United Kingdom - FTSE 100\": \"^FTSE\",\n",
    "    \"Japan - Nikkei 225\": \"^N225\",\n",
    "    \"Mexico - IPC Mexico\": \"^MXX\",\n",
    "    \"Brazil - Ibovespa\": \"^BVSP\",\n",
    "}\n",
    "\n",
    "start_date_ytd = \"2025-01-01\"\n",
    "end_date_ytd = \"2025-05-01\"\n",
    "\n",
    "ytd_returns = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12dbb3b3-535d-4cf6-a083-bfe251e3cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'United States - S&P 500': Ticker\n",
       " ^GSPC   -5.103301\n",
       " dtype: float64,\n",
       " 'China - Shanghai Composite': Ticker\n",
       " 000001.SS    0.504817\n",
       " dtype: float64,\n",
       " 'Hong Kong - HANG SENG INDEX': Ticker\n",
       " ^HSI    12.720018\n",
       " dtype: float64,\n",
       " 'Australia - S&P/ASX 200': Ticker\n",
       " ^AXJO   -0.9145\n",
       " dtype: float64,\n",
       " 'India - Nifty 50': Ticker\n",
       " ^NSEI    2.490424\n",
       " dtype: float64,\n",
       " 'Canada - S&P/TSX Composite': Ticker\n",
       " ^GSPTSE   -0.226126\n",
       " dtype: float64,\n",
       " 'Germany - DAX': Ticker\n",
       " ^GDAXI    12.346378\n",
       " dtype: float64,\n",
       " 'United Kingdom - FTSE 100': Ticker\n",
       " ^FTSE    2.84259\n",
       " dtype: float64,\n",
       " 'Japan - Nikkei 225': Ticker\n",
       " ^N225   -8.297931\n",
       " dtype: float64,\n",
       " 'Mexico - IPC Mexico': Ticker\n",
       " ^MXX    13.049444\n",
       " dtype: float64,\n",
       " 'Brazil - Ibovespa': Ticker\n",
       " ^BVSP    12.43871\n",
       " dtype: float64}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytd_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afe0a1a-cbda-4260-b51b-1c2e5eef3a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Year-to-Date Returns (as of May 1, 2025) ---\n",
      "Mexico - IPC Mexico            13.049444\n",
      "Hong Kong - HANG SENG INDEX    12.720018\n",
      "Brazil - Ibovespa              12.438710\n",
      "Germany - DAX                  12.346378\n",
      "United Kingdom - FTSE 100       2.842590\n",
      "India - Nifty 50                2.490424\n",
      "China - Shanghai Composite      0.504817\n",
      "Canada - S&P/TSX Composite     -0.226126\n",
      "Australia - S&P/ASX 200        -0.914500\n",
      "United States - S&P 500        -5.103301\n",
      "Japan - Nikkei 225             -8.297931\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for country, ticker in tickers.items():\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date_ytd, end=end_date_ytd, progress=False)\n",
    "\n",
    "        if not data.empty:\n",
    "            # Extract scalar values using .item() or .values[0]\n",
    "            initial_price = data['Close'].iloc[0].item() # Use .item() to get the scalar value\n",
    "            final_price = data['Close'].iloc[-1].item()   # Use .item() to get the scalar value\n",
    "\n",
    "            # Calculate YTD return\n",
    "            ytd_return = ((final_price - initial_price) / initial_price) * 100\n",
    "            ytd_returns[country] = ytd_return\n",
    "        else:\n",
    "            ytd_returns[country] = float('nan')\n",
    "            print(f\"No data found for {country} ({ticker}) in the YTD period.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        ytd_returns[country] = float('nan')\n",
    "        print(f\"Error downloading data for {country} ({ticker}): {e}\")\n",
    "\n",
    "# Convert to pandas Series, then drop NaN values for sorting and comparison\n",
    "ytd_returns_series = pd.Series(ytd_returns).dropna()\n",
    "\n",
    "print(\"\\n--- Year-to-Date Returns (as of May 1, 2025) ---\")\n",
    "print(ytd_returns_series.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc5075d-5e44-44a8-98d7-7e16fa98e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S&P 500 YTD Return: -5.10%\n",
      "Number of indexes with better YTD returns than S&P 500: 9\n"
     ]
    }
   ],
   "source": [
    "# Get S&P 500 YTD return\n",
    "sp500_ytd_return = ytd_returns.get(\"United States - S&P 500\")\n",
    "\n",
    "if isinstance(sp500_ytd_return, (int, float)) and not pd.isna(sp500_ytd_return):\n",
    "    better_than_sp500_ytd = 0\n",
    "    for country, ytd_return in ytd_returns_series.items(): # Iterate over the cleaned series\n",
    "        if country != \"United States - S&P 500\" and ytd_return > sp500_ytd_return:\n",
    "            better_than_sp500_ytd += 1\n",
    "    print(f\"\\nS&P 500 YTD Return: {sp500_ytd_return:.2f}%\")\n",
    "    print(f\"Number of indexes with better YTD returns than S&P 500: {better_than_sp500_ytd}\")\n",
    "else:\n",
    "    print(\"\\nCould not determine S&P 500 YTD return for comparison (it might be NaN or an error).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2da1635e-ce67-4dee-b04f-ec46782a7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Additional: 3, 5, and 10-year periods ---\n",
    "\n",
    "periods = {\n",
    "    \"3-year\": {\"start_date_offset\": 3},\n",
    "    \"5-year\": {\"start_date_offset\": 5},\n",
    "    \"10-year\": {\"start_date_offset\": 10},\n",
    "}\n",
    "\n",
    "comparison_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc041c3b-0550-4c33-aa83-231f4a1f72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3-year Returns (as of May 1, 2025) ---\n",
      "Germany - DAX                  61.395129\n",
      "India - Nifty 50               42.562875\n",
      "Japan - Nikkei 225             34.404756\n",
      "United States - S&P 500        34.020480\n",
      "Brazil - Ibovespa              26.658164\n",
      "Canada - S&P/TSX Composite     20.053451\n",
      "United Kingdom - FTSE 100      12.347091\n",
      "Australia - S&P/ASX 200        10.605692\n",
      "Mexico - IPC Mexico             8.425565\n",
      "China - Shanghai Composite      6.886816\n",
      "Hong Kong - HANG SENG INDEX     4.821935\n",
      "dtype: float64\n",
      "\n",
      "S&P 500 3-year Return: 34.02%\n",
      "Number of indexes with better 3-year returns than S&P 500: 3\n",
      "\n",
      "--- 5-year Returns (as of May 1, 2025) ---\n",
      "India - Nifty 50               161.841063\n",
      "Germany - DAX                  114.936570\n",
      "United States - S&P 500         96.737219\n",
      "Japan - Nikkei 225              83.723618\n",
      "Brazil - Ibovespa               71.239667\n",
      "Canada - S&P/TSX Composite      69.912379\n",
      "Australia - S&P/ASX 200         54.905743\n",
      "Mexico - IPC Mexico             54.684126\n",
      "United Kingdom - FTSE 100       47.401576\n",
      "China - Shanghai Composite      13.928827\n",
      "Hong Kong - HANG SENG INDEX     -6.328463\n",
      "dtype: float64\n",
      "\n",
      "S&P 500 5-year Return: 96.74%\n",
      "Number of indexes with better 5-year returns than S&P 500: 2\n",
      "\n",
      "--- 10-year Returns (as of May 1, 2025) ---\n",
      "India - Nifty 50               192.058866\n",
      "United States - S&P 500        164.150565\n",
      "Brazil - Ibovespa              135.497088\n",
      "Germany - DAX                   93.608190\n",
      "Japan - Nikkei 225              84.548741\n",
      "Canada - S&P/TSX Composite      61.942786\n",
      "Australia - S&P/ASX 200         39.759912\n",
      "Mexico - IPC Mexico             24.361595\n",
      "United Kingdom - FTSE 100       21.598918\n",
      "Hong Kong - HANG SENG INDEX    -21.349909\n",
      "China - Shanghai Composite     -26.814921\n",
      "dtype: float64\n",
      "\n",
      "S&P 500 10-year Return: 164.15%\n",
      "Number of indexes with better 10-year returns than S&P 500: 1\n",
      "\n",
      "--- Summary of Comparison Results ---\n",
      "For the 3-year period, 3 indexes had better returns than the S&P 500.\n",
      "For the 5-year period, 2 indexes had better returns than the S&P 500.\n",
      "For the 10-year period, 1 indexes had better returns than the S&P 500.\n",
      "\n",
      "--- Trend Analysis ---\n",
      "The number of indexes outperforming the S&P 500 decreases over longer time horizons.\n"
     ]
    }
   ],
   "source": [
    "for period_name, period_info in periods.items():\n",
    "    current_year = 2025\n",
    "    end_date = f\"{current_year}-05-01\"\n",
    "    start_year = current_year - period_info[\"start_date_offset\"]\n",
    "    start_date = f\"{start_year}-05-01\"\n",
    "\n",
    "    period_returns = {}\n",
    "    print(f\"\\n--- {period_name} Returns (as of May 1, 2025) ---\")\n",
    "\n",
    "    for country, ticker in tickers.items():\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "            if not data.empty:\n",
    "                # Extract scalar values using .item()\n",
    "                initial_price = data['Close'].iloc[0].item()\n",
    "                final_price = data['Close'].iloc[-1].item()\n",
    "\n",
    "                total_return = ((final_price - initial_price) / initial_price) * 100\n",
    "                period_returns[country] = total_return\n",
    "            else:\n",
    "                period_returns[country] = float('nan')\n",
    "                print(f\"No data found for {country} ({ticker}) in the {period_name} period.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            period_returns[country] = float('nan')\n",
    "            print(f\"Error downloading data for {country} ({ticker}): {e}\")\n",
    "\n",
    "    period_returns_series = pd.Series(period_returns).dropna()\n",
    "    print(period_returns_series.sort_values(ascending=False))\n",
    "\n",
    "    sp500_period_return = period_returns.get(\"United States - S&P 500\")\n",
    "    if isinstance(sp500_period_return, (int, float)) and not pd.isna(sp500_period_return):\n",
    "        better_than_sp500_period = 0\n",
    "        for country, p_return in period_returns_series.items(): # Iterate over the cleaned series\n",
    "            if country != \"United States - S&P 500\" and p_return > sp500_period_return:\n",
    "                better_than_sp500_period += 1\n",
    "        comparison_results[period_name] = better_than_sp500_period\n",
    "        print(f\"\\nS&P 500 {period_name} Return: {sp500_period_return:.2f}%\")\n",
    "        print(f\"Number of indexes with better {period_name} returns than S&P 500: {better_than_sp500_period}\")\n",
    "    else:\n",
    "        print(f\"\\nCould not determine S&P 500 {period_name} return for comparison (it might be NaN or an error).\")\n",
    "\n",
    "print(\"\\n--- Summary of Comparison Results ---\")\n",
    "for period, count in comparison_results.items():\n",
    "    print(f\"For the {period} period, {count} indexes had better returns than the S&P 500.\")\n",
    "\n",
    "print(\"\\n--- Trend Analysis ---\")\n",
    "if all(period in comparison_results for period in [\"3-year\", \"5-year\", \"10-year\"]):\n",
    "    count_3yr = comparison_results.get(\"3-year\", -1)\n",
    "    count_5yr = comparison_results.get(\"5-year\", -1)\n",
    "    count_10yr = comparison_results.get(\"10-year\", -1)\n",
    "\n",
    "    if count_3yr != -1 and count_5yr != -1 and count_10yr != -1:\n",
    "        if count_3yr > count_5yr and count_5yr > count_10yr:\n",
    "            print(\"The number of indexes outperforming the S&P 500 decreases over longer time horizons.\")\n",
    "        elif count_3yr < count_5yr and count_5yr < count_10yr:\n",
    "            print(\"The number of indexes outperforming the S&P 500 increases over longer time horizons.\")\n",
    "        elif count_3yr == count_5yr == count_10yr:\n",
    "            print(\"The trend is relatively consistent across the 3, 5, and 10-year periods.\")\n",
    "        else:\n",
    "            print(\"There is no clear consistent trend in the number of outperforming indexes across these periods.\")\n",
    "    else:\n",
    "        print(\"Not all period data is available for a comprehensive trend analysis.\")\n",
    "else:\n",
    "    print(\"Not all period data is available for a comprehensive trend analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae2e97-bfb0-482d-ba94-877b1d1aebe4",
   "metadata": {},
   "source": [
    "Question 3. [Index] S&P 500 Market Corrections Analysis\n",
    "Calculate the median duration (in days) of significant market corrections in the S&P 500 index.\n",
    "\n",
    "For this task, define a correction as an event when a stock index goes down by more than 5% from the closest all-time high maximum.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Download S&P 500 historical data (1950-present) using yfinance\n",
    "Identify all-time high points (where price exceeds all previous prices)\n",
    "For each pair of consecutive all-time highs, find the minimum price in between\n",
    "Calculate drawdown percentages: (high - low) / high × 100\n",
    "Filter for corrections with at least 5% drawdown\n",
    "Calculate the duration in days for each correction period\n",
    "Determine the 25th, 50th (median), and 75th percentiles for correction durations\n",
    "Context:\n",
    "\n",
    "Investors often wonder about the typical length of market corrections when deciding \"when to buy the dip\" (Reddit discussion).\n",
    "A Wealth of Common Sense - How Often Should You Expect a Stock Market Correction?\n",
    "Hint (use this data to compare with your results): Here is the list of top 10 largest corrections by drawdown:\n",
    "\n",
    "2007-10-09 to 2009-03-09: 56.8% drawdown over 517 days\n",
    "2000-03-24 to 2002-10-09: 49.1% drawdown over 929 days\n",
    "1973-01-11 to 1974-10-03: 48.2% drawdown over 630 days\n",
    "1968-11-29 to 1970-05-26: 36.1% drawdown over 543 days\n",
    "2020-02-19 to 2020-03-23: 33.9% drawdown over 33 days\n",
    "1987-08-25 to 1987-12-04: 33.5% drawdown over 101 days\n",
    "1961-12-12 to 1962-06-26: 28.0% drawdown over 196 days\n",
    "1980-11-28 to 1982-08-12: 27.1% drawdown over 622 days\n",
    "2022-01-03 to 2022-10-12: 25.4% drawdown over 282 days\n",
    "1966-02-09 to 1966-10-07: 22.2% drawdown over 240 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3c53015-69ed-4f5e-88d5-39f4e2c49b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading S&P 500 historical data (1950-present)...\n",
      "Data downloaded successfully. Shape: (18973, 8)\n",
      "\n",
      "First 5 rows of the data:\n",
      "             Open   High    Low  Close  Adj Close   Volume  Dividends  \\\n",
      "Date                                                                    \n",
      "1950-01-03  16.66  16.66  16.66  16.66      16.66  1260000        0.0   \n",
      "1950-01-04  16.85  16.85  16.85  16.85      16.85  1890000        0.0   \n",
      "1950-01-05  16.93  16.93  16.93  16.93      16.93  2550000        0.0   \n",
      "1950-01-06  16.98  16.98  16.98  16.98      16.98  2010000        0.0   \n",
      "1950-01-09  17.08  17.08  17.08  17.08      17.08  2520000        0.0   \n",
      "\n",
      "            Stock Splits  \n",
      "Date                      \n",
      "1950-01-03           0.0  \n",
      "1950-01-04           0.0  \n",
      "1950-01-05           0.0  \n",
      "1950-01-06           0.0  \n",
      "1950-01-09           0.0  \n",
      "\n",
      "Last 5 rows of the data:\n",
      "                   Open         High          Low        Close    Adj Close  \\\n",
      "Date                                                                          \n",
      "2025-05-23  5781.890137  5829.509766  5767.410156  5802.819824  5802.819824   \n",
      "2025-05-27  5854.069824  5924.330078  5854.069824  5921.540039  5921.540039   \n",
      "2025-05-28  5925.540039  5939.919922  5881.879883  5888.549805  5888.549805   \n",
      "2025-05-29  5939.959961  5943.129883  5873.799805  5912.169922  5912.169922   \n",
      "2025-05-30  5903.669922  5922.140137  5843.660156  5911.689941  5911.689941   \n",
      "\n",
      "                Volume  Dividends  Stock Splits  \n",
      "Date                                             \n",
      "2025-05-23  4662820000        0.0           0.0  \n",
      "2025-05-27  5366380000        0.0           0.0  \n",
      "2025-05-28  4665050000        0.0           0.0  \n",
      "2025-05-29  4569750000        0.0           0.0  \n",
      "2025-05-30  6378540000        0.0           0.0  \n",
      "\n",
      "Data information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 18973 entries, 1950-01-03 to 2025-05-30\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Open          18973 non-null  float64\n",
      " 1   High          18973 non-null  float64\n",
      " 2   Low           18973 non-null  float64\n",
      " 3   Close         18973 non-null  float64\n",
      " 4   Adj Close     18973 non-null  float64\n",
      " 5   Volume        18973 non-null  int64  \n",
      " 6   Dividends     18973 non-null  float64\n",
      " 7   Stock Splits  18973 non-null  float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading S&P 500 historical data (1950-present)...\")\n",
    "    \n",
    "sp500 = yf.Ticker(\"^GSPC\")\n",
    "# Fetch data using nominal prices (not adjusted for dividends for ATH calculation)\n",
    "hist = sp500.history(start=\"1950-01-01\", auto_adjust=False, back_adjust=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Data downloaded successfully. Shape: {hist.shape}\")\n",
    "    \n",
    "# Ensure the index is a DatetimeIndex and timezone-naive for simplicity\n",
    "if not isinstance(hist.index, pd.DatetimeIndex):\n",
    "        hist.index = pd.to_datetime(hist.index)\n",
    "if hist.index.tz is not None:\n",
    "    hist.index = hist.index.tz_localize(None)\n",
    "    \n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "print(hist.head())\n",
    "print(\"\\nLast 5 rows of the data:\")\n",
    "print(hist.tail())\n",
    "print(\"\\nData information:\")\n",
    "hist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "441bd385-35c4-4df1-b713-be40b682365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 1518 distinct all-time high setting days.\n",
      "\n",
      "Sample of identified ATH points (Date and Price):\n",
      "            ATH_Price\n",
      "Date                 \n",
      "1950-01-03      16.66\n",
      "1950-01-04      16.85\n",
      "1950-01-05      16.93\n",
      "1950-01-06      16.98\n",
      "1950-01-09      17.08\n",
      "\n",
      "Last few ATH points:\n",
      "              ATH_Price\n",
      "Date                   \n",
      "2025-01-22  6100.810059\n",
      "2025-01-23  6118.729980\n",
      "2025-01-24  6128.180176\n",
      "2025-02-18  6129.629883\n",
      "2025-02-19  6147.430176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the running all-time high based on the 'High' column\n",
    "hist['Running_ATH'] = hist['High'].cummax()\n",
    "\n",
    "# A new ATH is set if today's High is greater than the Running_ATH recorded up to the previous day.\n",
    "hist['Is_New_ATH'] = hist['High'] > hist['Running_ATH'].shift(1).fillna(0) \n",
    "    \n",
    "\n",
    "if not hist.empty and hist.iloc[0]['High'] == hist.iloc[0]['Running_ATH']:\n",
    "        hist.loc[hist.index[0], 'Is_New_ATH'] = True\n",
    "            \n",
    "ath_points_df = hist[hist['Is_New_ATH']].copy()\n",
    "\n",
    "\n",
    "ath_points_df = ath_points_df[['High']] # Keep only the 'High' column\n",
    "ath_points_df.rename(columns={'High': 'ATH_Price'}, inplace=True)\n",
    "\n",
    "print(f\"\\nIdentified {len(ath_points_df)} distinct all-time high setting days.\")\n",
    "\n",
    "print(\"\\nSample of identified ATH points (Date and Price):\")\n",
    "print(ath_points_df.head())\n",
    "print(\"\\nLast few ATH points:\")\n",
    "print(ath_points_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8c56d80-5401-41f3-863c-fff9c8d2c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1518 ATH points to find corrections.\n",
      "\n",
      "Found 96 significant corrections (>= 5% drawdown).\n",
      "\n",
      "Sample of significant corrections:\n",
      "    ATH_Date  ATH_Price Trough_Date  Trough_Price  Drawdown_Percent  \\\n",
      "0 1950-06-12  19.400000  1950-07-17     16.680000         14.020615   \n",
      "1 1950-11-24  20.320000  1950-12-04     19.000000          6.496062   \n",
      "2 1951-05-03  22.809999  1951-06-29     20.959999          8.110480   \n",
      "3 1951-10-15  23.850000  1951-11-23     22.400000          6.079668   \n",
      "4 1952-01-22  24.660000  1952-02-20     23.090000          6.366584   \n",
      "\n",
      "   Duration_Days  \n",
      "0             35  \n",
      "1             10  \n",
      "2             57  \n",
      "3             39  \n",
      "4             29  \n"
     ]
    }
   ],
   "source": [
    "corrections_data = []\n",
    "\n",
    "if not ath_points_df.empty and not hist.empty:\n",
    "    ath_dates_list = ath_points_df.index.tolist()\n",
    "\n",
    "    for i in range(len(ath_dates_list)):\n",
    "        ath_start_date = ath_dates_list[i]\n",
    "        ath_start_price = ath_points_df.loc[ath_start_date, 'ATH_Price']\n",
    "        \n",
    "        if i + 1 < len(ath_dates_list):\n",
    "            next_ath_date = ath_dates_list[i+1]\n",
    "            # Slice hist from current ATH up to, but not including, the next ATH date.\n",
    "            # Get the index position to slice correctly.\n",
    "            idx_current_ath = hist.index.get_loc(ath_start_date)\n",
    "            idx_next_ath = hist.index.get_loc(next_ath_date)\n",
    "            \n",
    "            # Period data is from current ATH up to the day before the next ATH\n",
    "            period_data = hist.iloc[idx_current_ath:idx_next_ath]\n",
    "        else:\n",
    "            # This is the last ATH, so the window extends to the end of the dataset\n",
    "            idx_current_ath = hist.index.get_loc(ath_start_date)\n",
    "            period_data = hist.iloc[idx_current_ath:]\n",
    "\n",
    "        if period_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Find the minimum 'Low' price in this period.\n",
    "        # The trough must occur at or after the ATH date.\n",
    "        min_low_in_period = period_data['Low'].min()\n",
    "        min_low_date = period_data['Low'].idxmin() # Gets the date of the first occurrence of the minimum\n",
    "\n",
    "        # Calculate drawdown: (ATH_price - min_low_price) / ATH_price\n",
    "        drawdown_percent = (ath_start_price - min_low_in_period) / ath_start_price * 100\n",
    "\n",
    "        # Filter for corrections with at least 5% drawdown\n",
    "        if drawdown_percent >= 5.0:\n",
    "            # Calculate duration in days for the correction period (ATH to trough)\n",
    "            duration_days = (min_low_date - ath_start_date).days\n",
    "            \n",
    "            corrections_data.append({\n",
    "                'ATH_Date': ath_start_date,\n",
    "                'ATH_Price': ath_start_price,\n",
    "                'Trough_Date': min_low_date,\n",
    "                'Trough_Price': min_low_in_period,\n",
    "                'Drawdown_Percent': drawdown_percent,\n",
    "                'Duration_Days': duration_days\n",
    "            })\n",
    "    print(f\"Processed {len(ath_dates_list)} ATH points to find corrections.\")\n",
    "else:\n",
    "    print(\"ATH points or historical data is empty. Cannot calculate corrections.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "significant_corrections_df = pd.DataFrame(corrections_data)\n",
    "\n",
    "if not significant_corrections_df.empty:\n",
    "    print(f\"\\nFound {len(significant_corrections_df)} significant corrections (>= 5% drawdown).\")\n",
    "    print(\"\\nSample of significant corrections:\")\n",
    "    print(significant_corrections_df.head())\n",
    "else:\n",
    "    print(\"\\nNo significant corrections (>= 5% drawdown) found with the current logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "916b643b-5c92-49bb-9192-bffb63433f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned and sorted significant corrections DataFrame:\n",
      "    ATH_Date  ATH_Price Trough_Date  Trough_Price  Drawdown_Percent  \\\n",
      "0 1950-06-12  19.400000  1950-07-17     16.680000         14.020615   \n",
      "1 1950-11-24  20.320000  1950-12-04     19.000000          6.496062   \n",
      "2 1951-05-03  22.809999  1951-06-29     20.959999          8.110480   \n",
      "3 1951-10-15  23.850000  1951-11-23     22.400000          6.079668   \n",
      "4 1952-01-22  24.660000  1952-02-20     23.090000          6.366584   \n",
      "\n",
      "   Duration_Days  \n",
      "0             35  \n",
      "1             10  \n",
      "2             57  \n",
      "3             39  \n",
      "4             29  \n",
      "     ATH_Date    ATH_Price Trough_Date  Trough_Price  Drawdown_Percent  \\\n",
      "91 2022-01-04  4818.620117  2022-10-13   3491.580078         27.539835   \n",
      "92 2024-03-28  5264.850098  2024-04-19   4953.560059          5.912610   \n",
      "93 2024-07-16  5669.669922  2024-08-05   5119.259766          9.707975   \n",
      "94 2024-12-06  6099.970215  2025-01-13   5773.310059          5.355111   \n",
      "95 2025-02-19  6147.430176  2025-04-07   4835.040039         21.348598   \n",
      "\n",
      "    Duration_Days  \n",
      "91            282  \n",
      "92             22  \n",
      "93             20  \n",
      "94             38  \n",
      "95             47  \n"
     ]
    }
   ],
   "source": [
    "if not significant_corrections_df.empty:\n",
    "    # Sort by ATH date for chronological order\n",
    "    significant_corrections_df.sort_values(by='ATH_Date', inplace=True)\n",
    "\n",
    "    print(\"\\nCleaned and sorted significant corrections DataFrame:\")\n",
    "    print(significant_corrections_df.head())\n",
    "    print(significant_corrections_df.tail())\n",
    "\n",
    "else:\n",
    "    print(\"No significant corrections to analyze further.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58badea2-e827-47d1-b4a1-8ffccf369eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 largest corrections by drawdown percentage:\n",
      "      ATH_Date Trough_Date  Drawdown_Percent  Duration_Days\n",
      "74  2007-10-11  2009-03-06         57.693406            512\n",
      "72  2000-03-24  2002-10-10         50.502617            930\n",
      "26  1973-01-11  1974-10-04         49.926072            631\n",
      "22  1968-12-02  1970-05-26         37.267990            540\n",
      "44  1987-08-25  1987-10-20         35.937732             56\n",
      "86  2020-02-19  2020-03-23         35.410426             33\n",
      "15  1961-12-12  1962-06-25         29.308922            195\n",
      "30  1980-11-26  1982-08-09         28.007895            621\n",
      "91  2022-01-04  2022-10-13         27.539835            282\n",
      "18  1966-02-09  1966-10-10         23.690881            243\n"
     ]
    }
   ],
   "source": [
    "if not significant_corrections_df.empty:\n",
    "    print(\"\\nTop 10 largest corrections by drawdown percentage:\")\n",
    "    top_10_drawdowns = significant_corrections_df.sort_values(by='Drawdown_Percent', ascending=False).head(10)\n",
    "    \n",
    "    # Format dates for better readability\n",
    "    top_10_drawdowns_display = top_10_drawdowns.copy()\n",
    "    top_10_drawdowns_display['ATH_Date'] = top_10_drawdowns_display['ATH_Date'].dt.strftime('%Y-%m-%d')\n",
    "    top_10_drawdowns_display['Trough_Date'] = top_10_drawdowns_display['Trough_Date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(top_10_drawdowns_display[['ATH_Date', 'Trough_Date', 'Drawdown_Percent', 'Duration_Days']])\n",
    "else:\n",
    "    print(\"No significant corrections to display top 10 from.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bc3fe66-aa03-400e-8606-211fd405278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- S&P 500 Market Correction Duration Analysis (1950-Present) ---\n",
      "Total significant corrections (>= 5% drawdown) found: 96\n",
      "\n",
      "Percentiles for correction durations (from ATH peak to trough bottom):\n",
      "  25th percentile: 15 days\n",
      "  50th percentile (Median): 33 days\n",
      "  75th percentile: 65 days\n",
      "\n",
      "ANSWER TO QUESTION 3 (as per calculation):\n",
      "The median duration (50th percentile) of significant market corrections in the S&P 500 index is 33 days.\n"
     ]
    }
   ],
   "source": [
    "if not significant_corrections_df.empty:\n",
    "    durations = significant_corrections_df['Duration_Days']\n",
    "    \n",
    "    if durations.empty or durations.isnull().all(): # Check if durations series is empty or all NaN\n",
    "        print(\"\\nNo valid correction durations to calculate percentiles from.\")\n",
    "    else:\n",
    "        percentile_25 = durations.quantile(0.25)\n",
    "        median_duration = durations.quantile(0.50) # This is the 50th percentile\n",
    "        percentile_75 = durations.quantile(0.75)\n",
    "\n",
    "        print(\"\\n--- S&P 500 Market Correction Duration Analysis (1950-Present) ---\")\n",
    "        print(f\"Total significant corrections (>= 5% drawdown) found: {len(significant_corrections_df)}\")\n",
    "        \n",
    "        print(\"\\nPercentiles for correction durations (from ATH peak to trough bottom):\")\n",
    "        print(f\"  25th percentile: {percentile_25:.0f} days\")\n",
    "        print(f\"  50th percentile (Median): {median_duration:.0f} days\")\n",
    "        print(f\"  75th percentile: {percentile_75:.0f} days\")\n",
    "        \n",
    "        print(f\"\\nANSWER TO QUESTION 3 (as per calculation):\")\n",
    "        print(f\"The median duration (50th percentile) of significant market corrections in the S&P 500 index is {median_duration:.0f} days.\")\n",
    "else:\n",
    "    print(\"\\nNo significant corrections DataFrame available to calculate duration percentiles.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ced752-f370-42c5-87de-db11dabc652e",
   "metadata": {},
   "source": [
    "Question 4. [Stocks] Earnings Surprise Analysis for Amazon (AMZN)\n",
    "Calculate the median 2-day percentage change in stock prices following positive earnings surprises days.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Load earnings data from CSV (ha1_Amazon.csv) containing earnings dates, EPS estimates, and actual EPS. Make sure you are using the correct delimiter to read the data, such as in this command python pandas.read_csv(\"ha1_Amazon.csv\", delimiter=';') \n",
    "Download complete historical price data using yfinance\n",
    "Calculate 2-day percentage changes for all historical dates: for each sequence of 3 consecutive trading days (Day 1, Day 2, Day 3), compute the return as Close_Day3 / Close_Day1 - 1. (Assume Day 2 may correspond to the earnings announcement.)\n",
    "Identify positive earnings surprises (where \"actual EPS > estimated EPS\" OR \"Surprise (%)>0\")\n",
    "Calculate 2-day percentage changes following positive earnings surprises. Show your answer in % (closest number to the 2nd digit): return * 100.0\n",
    "(Optional) Compare the median 2-day percentage change for positive surprises vs. all historical dates. Do you see the difference?\n",
    "Context: Earnings announcements, especially when they exceed analyst expectations, can significantly impact stock prices in the short term.\n",
    "\n",
    "Reference: Yahoo Finance earnings calendar - https://finance.yahoo.com/calendar/earnings?symbol=AMZN\n",
    "\n",
    "Additional: Is there a correlation between the magnitude of the earnings surprise and the stock price reaction? Does the market react differently to earnings surprises during bull vs. bear markets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a74714af-ae9e-4c5a-a8b9-8668e8b9b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Load and Prepare Earnings Data ---\n",
      "Loaded ha1_Amazon.csv. Original columns: ['Symbol', 'Company', 'Earnings Date', 'EPS Estimate', 'Reported EPS', 'Surprise (%)']\n",
      "Cleaned columns: ['Symbol', 'Company', 'Earnings Date', 'EPS Estimate', 'Reported EPS', 'Surprise (%)']\n",
      "\n",
      "Processed Earnings Data (Sample):\n",
      "  Symbol           Company Earnings Date  EPS Estimate  Reported EPS  \\\n",
      "0   AMZN    Amazon.com Inc    2026-04-29           NaN           NaN   \n",
      "1   AMZN    Amazon.com Inc    2026-02-04           NaN           NaN   \n",
      "2   AMZN    Amazon.com Inc    2025-10-29           NaN           NaN   \n",
      "3   AMZN    Amazon.com Inc    2025-07-30           NaN           NaN   \n",
      "4   AMZN  Amazon.com, Inc.    2025-05-01           NaN           NaN   \n",
      "\n",
      "   Surprise (%)  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4         16.74  \n",
      "\n",
      "Earnings Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117 entries, 0 to 116\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Symbol         116 non-null    object        \n",
      " 1   Company        116 non-null    object        \n",
      " 2   Earnings Date  116 non-null    datetime64[ns]\n",
      " 3   EPS Estimate   84 non-null     float64       \n",
      " 4   Reported EPS   92 non-null     float64       \n",
      " 5   Surprise (%)   112 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(2)\n",
      "memory usage: 5.6+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zclxs\\AppData\\Local\\Temp\\ipykernel_19312\\2500148976.py:22: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(earnings_df['Earnings Date']):\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Load and Prepare Earnings Data ---\")\n",
    "earnings_csv_path = \"ha1_Amazon.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "earnings_df = pd.read_csv(earnings_csv_path, delimiter=';', encoding='utf-8')\n",
    "print(f\"Loaded {earnings_csv_path}. Original columns: {earnings_df.columns.tolist()}\")\n",
    "\n",
    "# Strip whitespace from column headers\n",
    "earnings_df.columns = earnings_df.columns.str.strip()\n",
    "print(f\"Cleaned columns: {earnings_df.columns.tolist()}\")\n",
    "\n",
    "# Define the target name for the surprise column\n",
    "target_surprise_col_name = 'Surprise (%)'\n",
    "\n",
    "# Process 'Earnings Date'\n",
    "if 'Earnings Date' in earnings_df.columns:\n",
    "    earnings_df['Earnings Date'] = earnings_df['Earnings Date'].astype(str).str.split(' at ').str[0]\n",
    "    earnings_df['Earnings Date'] = pd.to_datetime(earnings_df['Earnings Date'], errors='coerce')\n",
    "    if pd.api.types.is_datetime64tz_dtype(earnings_df['Earnings Date']):\n",
    "        earnings_df['Earnings Date'] = earnings_df['Earnings Date'].dt.tz_localize(None)\n",
    "else:\n",
    "    print(\"Critical Error: 'Earnings Date' column not found.\")\n",
    "\n",
    "columns_to_convert = ['EPS Estimate', 'Reported EPS', target_surprise_col_name]\n",
    "for col in columns_to_convert:\n",
    "    if col in earnings_df.columns:\n",
    "        if earnings_df[col].dtype == 'object': \n",
    "            earnings_df[col] = earnings_df[col].astype(str).str.replace(',', '.', regex=False)\n",
    "        earnings_df[col] = pd.to_numeric(earnings_df[col], errors='coerce')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' for numeric conversion not found.\")\n",
    "\n",
    "print(\"\\nProcessed Earnings Data (Sample):\")\n",
    "print(earnings_df.head())\n",
    "print(\"\\nEarnings Data Info:\")\n",
    "earnings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a448161-1d1d-4c18-9e6d-f11469c14bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Download AMZN Historical Price Data ---\n",
      "\n",
      "Historical Price Data for AMZN (Sample):\n",
      "                Open      High       Low     Close      Volume  Dividends  \\\n",
      "Date                                                                        \n",
      "1997-05-15  0.121875  0.125000  0.096354  0.097917  1443120000        0.0   \n",
      "1997-05-16  0.098438  0.098958  0.085417  0.086458   294000000        0.0   \n",
      "1997-05-19  0.088021  0.088542  0.081250  0.085417   122136000        0.0   \n",
      "1997-05-20  0.086458  0.087500  0.081771  0.081771   109344000        0.0   \n",
      "1997-05-21  0.081771  0.082292  0.068750  0.071354   377064000        0.0   \n",
      "\n",
      "            Stock Splits  \n",
      "Date                      \n",
      "1997-05-15           0.0  \n",
      "1997-05-16           0.0  \n",
      "1997-05-19           0.0  \n",
      "1997-05-20           0.0  \n",
      "1997-05-21           0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Download AMZN Historical Price Data ---\")\n",
    "amzn = yf.Ticker(\"AMZN\")\n",
    "hist_prices = amzn.history(period=\"max\", auto_adjust=True)\n",
    "\n",
    "hist_prices.index = pd.to_datetime(hist_prices.index).tz_localize(None)\n",
    "\n",
    "print(\"\\nHistorical Price Data for AMZN (Sample):\")\n",
    "print(hist_prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b398c210-9d82-43d7-abbc-77b0e2a9763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Calculate 2-Day Percentage Changes for All Historical Dates ---\n",
      "\n",
      "Historical prices with 2-day percentage changes (Sample):\n",
      "               Close  Close_Day1  Close_Day3  2_Day_Pct_Change\n",
      "Date                                                          \n",
      "1997-05-15  0.097917         NaN    0.086458               NaN\n",
      "1997-05-16  0.086458    0.097917    0.085417        -12.765910\n",
      "1997-05-19  0.085417    0.086458    0.081771         -5.421125\n",
      "1997-05-20  0.081771    0.085417    0.071354        -16.463936\n",
      "1997-05-21  0.071354    0.081771    0.069792        -14.649446\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Calculate 2-Day Percentage Changes for All Historical Dates ---\")\n",
    "hist_prices['Close_Day1'] = hist_prices['Close'].shift(1)\n",
    "hist_prices['Close_Day3'] = hist_prices['Close'].shift(-1)\n",
    "hist_prices['2_Day_Pct_Change'] = (hist_prices['Close_Day3'] / hist_prices['Close_Day1'] - 1) * 100.0\n",
    "\n",
    "print(\"\\nHistorical prices with 2-day percentage changes (Sample):\")\n",
    "print(hist_prices[['Close', 'Close_Day1', 'Close_Day3', '2_Day_Pct_Change']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da3c170f-e50a-46a8-9ceb-e531255867fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Identify Positive Earnings Surprises ---\n",
      "\n",
      "Identified 86 positive earnings surprises.\n",
      "Positive Surprise Dates (Sample):\n",
      "  Earnings Date  EPS Estimate  Reported EPS  Surprise (%)\n",
      "4    2025-05-01           NaN           NaN         16.74\n",
      "5    2025-02-06           NaN           NaN         24.47\n",
      "6    2024-10-31           NaN           NaN         25.17\n",
      "7    2024-08-01           NaN           NaN         22.58\n",
      "8    2024-04-30          0.83          0.98         17.91\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Identify Positive Earnings Surprises ---\")\n",
    "target_surprise_col_name = 'Surprise (%)' # Ensure consistency\n",
    "\n",
    "# Conditions for positive surprise\n",
    "# Assumes 'Reported EPS', 'EPS Estimate', and target_surprise_col_name exist and are numeric\n",
    "condition1 = earnings_df['Reported EPS'] > earnings_df['EPS Estimate']\n",
    "condition2 = earnings_df[target_surprise_col_name].fillna(0) > 0 # Fill NaN with 0 for comparison\n",
    "\n",
    "positive_surprises = earnings_df[condition1 | condition2].copy()\n",
    "\n",
    "print(f\"\\nIdentified {len(positive_surprises)} positive earnings surprises.\")\n",
    "if not positive_surprises.empty:\n",
    "    print(\"Positive Surprise Dates (Sample):\")\n",
    "    # Display relevant columns, assuming they exist in positive_surprises\n",
    "    display_cols = ['Earnings Date', 'EPS Estimate', 'Reported EPS', target_surprise_col_name]\n",
    "    # Filter display_cols to only those present in positive_surprises to avoid KeyErrors\n",
    "    display_cols = [col for col in display_cols if col in positive_surprises.columns]\n",
    "    if display_cols:\n",
    "        print(positive_surprises[display_cols].head())\n",
    "    else:\n",
    "        print(\"No relevant columns to display in positive_surprises. Printing raw head:\")\n",
    "        print(positive_surprises.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6580b66c-eb66-40ea-a780-e21949ba8ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Calculate 2-Day Percentage Changes Following Positive Earnings Surprises ---\n",
      "\n",
      "Found 86 positive surprise events with corresponding price changes.\n",
      "2-Day Pct Change for Positive Surprises (Sample):\n",
      "  Earnings Date  Reported EPS  EPS Estimate  Surprise (%)  2_Day_Pct_Change\n",
      "4    2025-05-01           NaN           NaN         16.74          3.014856\n",
      "5    2025-02-06           NaN           NaN         24.47         -2.972437\n",
      "6    2024-10-31           NaN           NaN         25.17          2.698074\n",
      "7    2024-08-01           NaN           NaN         22.58        -10.204301\n",
      "8    2024-04-30          0.98          0.83         17.91         -1.083116\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 5: Calculate 2-Day Percentage Changes Following Positive Earnings Surprises ---\")\n",
    "# Merge positive surprise dates with historical prices\n",
    "merged_data = pd.merge(positive_surprises,\n",
    "                       hist_prices[['2_Day_Pct_Change']],\n",
    "                       left_on='Earnings Date',\n",
    "                       right_index=True,\n",
    "                       how='inner')\n",
    "\n",
    "# Drop rows where the 2-day percentage change couldn't be calculated\n",
    "merged_data.dropna(subset=['2_Day_Pct_Change'], inplace=True)\n",
    "\n",
    "print(f\"\\nFound {len(merged_data)} positive surprise events with corresponding price changes.\")\n",
    "if not merged_data.empty:\n",
    "    print(\"2-Day Pct Change for Positive Surprises (Sample):\")\n",
    "    # Display relevant columns, assuming they exist in merged_data\n",
    "    display_cols_merged = ['Earnings Date', 'Reported EPS', 'EPS Estimate', target_surprise_col_name, '2_Day_Pct_Change']\n",
    "    display_cols_merged = [col for col in display_cols_merged if col in merged_data.columns]\n",
    "    if display_cols_merged:\n",
    "        print(merged_data[display_cols_merged].head())\n",
    "    else:\n",
    "        print(\"No relevant columns to display in merged_data. Printing raw head:\")\n",
    "        print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6bbc4ff5-c9d5-4df9-8677-e2b37f02acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Calculate Median 2-Day Percentage Change Following Positive Surprises ---\n",
      "\n",
      ">>> The median 2-day percentage change following positive surprises is: 1.04%\n",
      "\n",
      "--- Step 7: (Optional) Compare with All Historical 2-Day Changes ---\n",
      "The median 2-day percentage change for all historical dates is: 0.16%\n",
      "Difference (Positive Surprise Median - All Historical Median): 0.88%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Calculate Median 2-Day Percentage Change Following Positive Surprises ---\")\n",
    "if not merged_data.empty and '2_Day_Pct_Change' in merged_data.columns:\n",
    "    median_surprise_change = merged_data['2_Day_Pct_Change'].median()\n",
    "    print(f\"\\n>>> The median 2-day percentage change following positive surprises is: {median_surprise_change:.2f}%\")\n",
    "else:\n",
    "    median_surprise_change = np.nan # Set to NaN if not calculable\n",
    "    print(\"\\nCould not calculate median for positive surprises (no data or column missing).\")\n",
    "\n",
    "print(\"\\n--- Step 7: (Optional) Compare with All Historical 2-Day Changes ---\")\n",
    "if not hist_prices.empty and '2_Day_Pct_Change' in hist_prices.columns and hist_prices['2_Day_Pct_Change'].notna().any():\n",
    "    median_all_historical_changes = hist_prices['2_Day_Pct_Change'].median()\n",
    "    print(f\"The median 2-day percentage change for all historical dates is: {median_all_historical_changes:.2f}%\")\n",
    "\n",
    "    if not np.isnan(median_surprise_change) and not np.isnan(median_all_historical_changes):\n",
    "        difference = median_surprise_change - median_all_historical_changes\n",
    "        print(f\"Difference (Positive Surprise Median - All Historical Median): {difference:.2f}%\")\n",
    "    else:\n",
    "        print(\"Cannot compute difference due to missing median values.\")\n",
    "else:\n",
    "    print(\"Could not calculate median for all historical changes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6277c-f968-47be-9771-e61d0780ec40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
